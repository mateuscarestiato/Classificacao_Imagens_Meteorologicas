# ğŸ“˜ ClassificaÃ§Ã£o de CondiÃ§Ãµes ClimÃ¡ticas com Deep Learning
EfficientNetB0 + Transfer Learning + Fine-Tuning

Este repositÃ³rio contÃ©m um projeto completo de classificaÃ§Ã£o de imagens meteorolÃ³gicas, utilizando Deep Learning com Transfer Learning e fine-tuning parcial. O objetivo Ã© identificar automaticamente condiÃ§Ãµes climÃ¡ticas em imagens externas, classificando-as em:

â˜ï¸ Cloudy (Nublado)

ğŸŒ Sunny (Ensolarado)

ğŸŒ§ï¸ Rain (Chuvoso)

ğŸŒ… Sunrise (Nascer do Sol)

O projeto foi desenvolvido em Python utilizando TensorFlow/Keras, com foco em execuÃ§Ã£o no Google Colab.

ğŸ§  1. IntroduÃ§Ã£o

A classificaÃ§Ã£o automÃ¡tica de condiÃ§Ãµes meteorolÃ³gicas a partir de imagens Ã© extremamente Ãºtil em:

- Monitoramento de trÃ¡fego
- Sistemas de planejamento urbano
- PrevisÃ£o meteorolÃ³gica assistida
- AutomaÃ§Ã£o industrial
- VeÃ­culos autÃ´nomos

Para realizar a tarefa, empregamos:

- Transfer Learning com EfficientNetB0
- Data augmentation para ampliar robustez
- Treinamento hÃ­brido (feature extraction â†’ fine-tuning)
- MÃ©tricas profissionais (train/val/test)
- A arquitetura EfficientNetB0 foi escolhida por fornecer o melhor equilÃ­brio entre:
- Qualidade de representaÃ§Ã£o visual
- Velocidade de inferÃªncia
- Risco reduzido de overfitting
- Baixa complexidade computacional

ğŸ¯ 2. Objetivo

Construir um modelo capaz de classificar imagens em quatro categorias climÃ¡ticas utilizando:
- TensorFlow / Keras
- Transfer Learning
- Pipeline de dados otimizado
- Treinamento em duas fases
- Fine-tuning

ğŸ“¦ 3. Dataset

ğŸ“ Nome: Multi-class Weather Dataset

ğŸ”— Download: (via Google Drive)

https://drive.google.com/file/d/10eg72mzwrhK0b5RDEqBg1XgOVWwZ8WTA/view

ğŸ·ï¸ Classes: Cloudy, Rain, Sunny, Sunrise

ğŸ“¸ Tamanho: ~1100 imagens

ğŸ—‚ï¸ Estrutura dos diretÃ³rios:

Multi-class Weather Dataset/
 â”œâ”€â”€ Cloudy/
 â”œâ”€â”€ Rain/
 â”œâ”€â”€ Sunny/
 â””â”€â”€ Sunrise/

âš™ï¸ 4. InstalaÃ§Ã£o e ExecuÃ§Ã£o
â–¶ï¸ ExecuÃ§Ã£o no Google Colab (recomendado)

Abra o notebook.

Ative GPU em: Runtime â†’ Change runtime type â†’ GPU.

Execute as cÃ©lulas na ordem.

ğŸ’» ExecuÃ§Ã£o local

pip install tensorflow numpy matplotlib seaborn scikit-learn gdown

Baixe o dataset manualmente e ajuste os caminhos, se necessÃ¡rio.

ğŸ“¥ 5. Download e ExtraÃ§Ã£o AutomÃ¡tica do Dataset

!pip install gdown

!gdown --id 10eg72mzwrhK0b5RDEqBg1XgOVWwZ8WTA -O weather.zip

import zipfile, os

zip_path = "weather.zip"

extract_path = "weather_dataset"

os.makedirs(extract_path, exist_ok=True)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:

    zip_ref.extractall(extract_path)

print("ExtraÃ§Ã£o concluÃ­da!")

ğŸ§­ 6. Carregamento do Dataset (Treino, ValidaÃ§Ã£o e Teste)

DivisÃ£o utilizada:

70% â†’ Treino

20% â†’ ValidaÃ§Ã£o

10% â†’ Teste

import tensorflow as tf

import os

base_dir = "/content/weather_dataset/Multi-class Weather Dataset"

batch_size = 32

img_size = (224, 224)

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    base_dir,
    validation_split=0.30,
    subset="training",
    seed=123,
    image_size=img_size,
    batch_size=batch_size
)

temp_ds = tf.keras.preprocessing.image_dataset_from_directory(
    base_dir,
    validation_split=0.30,
    subset="validation",
    seed=123,
    image_size=img_size,
    batch_size=batch_size
)

class_names = train_ds.class_names
print("Classes:", class_names)

val_size = 0.66   # 20% val + 10% test

val_ds = temp_ds.take(int(len(temp_ds) * val_size))

test_ds = temp_ds.skip(int(len(temp_ds) * val_size))

ğŸš€ 7. OtimizaÃ§Ã£o do Pipeline

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(AUTOTUNE)

val_ds = val_ds.prefetch(AUTOTUNE)

test_ds = test_ds.prefetch(AUTOTUNE)

ğŸ”„ 8. Data Augmentation
from tensorflow.keras import layers, Sequential

data_augmentation = Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2),
])

ğŸ§© 9. Modelo: EfficientNetB0 + CabeÃ§ote

from tensorflow import keras

base_model = keras.applications.EfficientNetB0(
    include_top=False,
    input_shape=img_size + (3,),
    weights="imagenet"
)

base_model.trainable = False

inputs = keras.Input(shape=img_size + (3,))

x = data_augmentation(inputs)

x = keras.applications.efficientnet.preprocess_input(x)

x = base_model(x, training=False)

x = layers.GlobalAveragePooling2D()(x)

x = layers.Dropout(0.3)(x)

outputs = layers.Dense(4, activation="softmax")(x)

model = keras.Model(inputs, outputs)

ğŸ‹ï¸â€â™‚ï¸ 10. Treinamento â€“ Fase 1 (Feature Extraction)

model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(
    train_ds,
    epochs=10,
    validation_data=val_ds
)

ğŸ”§ 11. Fine-Tuning (Fase 2)

Apenas as Ãºltimas camadas da EfficientNet sÃ£o destravadas.

base_model.trainable = True

for layer in base_model.layers[:150]:
    layer.trainable = False

model.compile(
    optimizer=keras.optimizers.Adam(1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history_finetune = model.fit(
    train_ds,
    epochs=10,
    validation_data=val_ds
)

ğŸ“Š 12. AvaliaÃ§Ã£o Final no Conjunto de Teste

test_loss, test_acc = model.evaluate(test_ds)

print("AcurÃ¡cia no conjunto de teste:", test_acc)


Para mÃ©tricas detalhadas:

from sklearn.metrics import confusion_matrix, classification_report

import numpy as np

y_true = []

y_pred = []

for images, labels in test_ds:
    preds = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

print(classification_report(y_true, y_pred, target_names=class_names))

ğŸ§¾ 13. ConclusÃ£o

A estratÃ©gia adotada se mostrou altamente eficaz, pois:

âœ” Transfer Learning reduz os requisitos de dados

âœ” EfficientNetB0 extrai padrÃµes visuais sofisticados

âœ” Fine-tuning permite especializar o modelo no domÃ­nio meteorolÃ³gico

âœ” Data augmentation reduz overfitting

âœ” DivisÃ£o 70/20/10 garante avaliaÃ§Ã£o confiÃ¡vel

Resultado: modelo leve, rÃ¡pido e com excelente acurÃ¡cia, ideal para aplicaÃ§Ãµes reais.

ğŸ“š 14. ReferÃªncias

- Modelos e Deep Learning

- Chollet, F. Deep Learning with Python. Manning, 2017.

- TensorFlow. Transfer Learning & Fine-Tuning Documentation.

- Krizhevsky, A. et al. â€œImageNet Classification with Deep CNNsâ€. NIPS, 2012.

- Sandler, M. et al. â€œMobileNetV2â€. Google Research, 2018.

- Suporte com IA (prompts utilizados)

- ComparaÃ§Ã£o tÃ©cnica entre arquiteturas (MobileNetV2, ResNet50, EfficientNetB0) para condiÃ§Ãµes climÃ¡ticas.

- GeraÃ§Ã£o de cÃ³digo para carregar dataset zipado via Google Drive.

- CÃ³digo inicial de anÃ¡lise e pipeline de classificaÃ§Ã£o gerado via IA.

ğŸ“Œ 15. PossÃ­veis ExtensÃµes

- ExportaÃ§Ã£o do modelo (model.save("weather_classifier.keras"))
- API para inferÃªncia (Flask/FastAPI)
- Dashboard visual
- Testes com EfficientNetB1â€“B3
- Early stopping e checkpoints
- ExpansÃ£o para 10+ classes climÃ¡ticas
